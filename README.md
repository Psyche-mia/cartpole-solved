Python 3.6 + PyTorch

This implementation is **not** designed for high performance and flexibility. These are the main DQN extensions in a simple and clean code, solving classic `CartPole` problem. It's basic playground for understanding algorithms, supplementary material for some [great](https://github.com/yandexdataschool/Practical_RL) [RL](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) [course](http://rail.eecs.berkeley.edu/deeprlcourse/).

Notebooks work on Colaboratory, no setup required.
Open and train from scratch, it takes one minute.

1. Double Dueling DQN [colab](https://colab.research.google.com/github/htdt/cartpole-solved/blob/master/dqn.ipynb)
1. Prioritized Experience Replay [colab](https://colab.research.google.com/github/htdt/cartpole-solved/blob/master/prior.ipynb)
1. DQN with GRU RNN and n-step updates [colab](https://colab.research.google.com/github/htdt/cartpole-solved/blob/master/recurrent.ipynb)
1. Implicit Quantile Networks (IQN) [colab](https://colab.research.google.com/github/htdt/cartpole-solved/blob/master/iqn.ipynb) [paper](https://arxiv.org/abs/1806.06923)

See also:
- [higgsfield/RL-Adventure](https://github.com/higgsfield/RL-Adventure)
- [google/dopamine](https://github.com/google/dopamine)
